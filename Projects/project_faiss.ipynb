{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b3a34b6",
   "metadata": {},
   "source": [
    "## 🔹 Install & Import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc1da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136712bc",
   "metadata": {},
   "source": [
    "## 🔹 Load & Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b34d9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FILE_PATH = r'/Users/sahiljain/Data Science/Course GenAI/Projects/Forecasting Models.pdf'\n",
    "\n",
    "loader = PyPDFLoader(FILE_PATH)\n",
    "pages = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = splitter.split_documents(pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538fd54",
   "metadata": {},
   "source": [
    "## 🔹 Create FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "936cc9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build FAISS index from docs\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Save locally for reuse\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "# Reload when needed\n",
    "vector_store = FAISS.load_local(\n",
    "    \"faiss_index\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Retriever setup\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 10, \"lambda_mult\": 0.5}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62bccc",
   "metadata": {},
   "source": [
    "## 🔹 Retrieval + LLM QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5acdbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/6kym10_j1sn5_405c08t9hlh0000gn/T/ipykernel_48931/474378644.py:35: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Answer:\n",
      "\n",
      "ARIMA (Autoregressive Integrated Moving Average) models are statistical models used for analyzing\n",
      "and forecasting time series data.  They capture patterns in data by combining autoregressive (AR),\n",
      "integrated (I), and moving average (MA) components.  The 'I' component involves differencing the\n",
      "data to achieve stationarity (a constant mean and variance over time).  The AR component models the\n",
      "relationship between the current value and past values, while the MA component models the\n",
      "relationship between the current value and past forecast errors.  Seasonal ARIMA models extend the\n",
      "basic ARIMA model to incorporate seasonal patterns.  These are denoted as ARIMA(p,d,q)(P,D,Q)m,\n",
      "where the lowercase letters refer to non-seasonal components and the uppercase letters refer to\n",
      "seasonal components, with 'm' representing the seasonal period.  For example, ARIMA(1,1,1)(1,1,1)4\n",
      "indicates a model with non-seasonal and seasonal AR(1), I(1), MA(1) components and a seasonal period\n",
      "of 4.  Model selection often involves using information criteria like AICc (Corrected Akaike\n",
      "Information Criterion) to compare different models and choose the one with the best fit.  Stepwise\n",
      "procedures, such as those employed by the `auto.arima()` function in R, systematically explore\n",
      "different ARIMA model structures to find the one that minimizes the AICc.  Diagnostic checks, such\n",
      "as examining autocorrelation and partial autocorrelation functions (ACF and PACF) of the residuals,\n",
      "are crucial to assess the adequacy of the chosen model.  SOURCES: Pages 78, 84, 85\n",
      "\n",
      "📚 Sources:\n",
      "1. Page: 77 — Forecasting: principles and practice 78 7.5 ARIMA modelling in R auto.arima(): Hyndman and Khandakar (JSS, 2008) algorithm • Select no. di ﬀerences dand Dvia unit root tests. • Select p,q by minimising AICc. • Use stepwise search to traverse model space. Step 1: Select current model (with smallest A...\n",
      "2. Page: 83 — 8 Seasonal ARIMA models ARIMA ( p,d,q)      (P,D,Q )m              where m= number of periods per season. ↑ ↑  Non- seasonal part of the model     Seasonal part of the model   E.g., ARIMA(1,1,1)(1,1,1)4 model (without constant) (1 −φ1B)(1 −Φ1B4)(1 ...\n",
      "3. Page: 105 — Solution: Begin with a proxy model for the ARIMA errors. • Assume AR(2) model for for non-seasonal data; • Assume ARIMA(2,0,0)(1,0,0) m model for seasonal data. Estimate model, determine better error structure, and re-estimate. 1. Check that all variables are stationary. If not, apply diﬀerencing. W...\n",
      "4. Page: 79 — ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 0 5 10 15 20 25 30 35 −0.3 −0.1 0.1 0.3 Lag ACF 0 5 10 15 20 25 30 35 −0.3 −0.1 0.1 0.3 Lag PACF > tsdisplay(diff(eeadj)) 4. PACF is suggestive of AR(3). So initial candidate model is ARIMA(3,1,0). No other obvious candida...\n",
      "5. Page: 84 — Forecasting: principles and practice 85 8.1 Common ARIMA models In the US Census Bureau uses the following models most often: ARIMA(0,1,1)(0,1,1)m with log transformation ARIMA(0,1,2)(0,1,1)m with log transformation ARIMA(2,1,0)(0,1,1)m with log transformation ARIMA(0,2,2)(0,1,1)m with log transform...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import textwrap\n",
    "\n",
    "# Custom Prompt\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are an assistant. Use the following CONTEXT to answer the QUESTION.\\n\\n\"\n",
    "        \"CONTEXT:\\n{context}\\n\\n\"\n",
    "        \"QUESTION:\\n{question}\\n\\n\"\n",
    "        \"INSTRUCTIONS:\\n\"\n",
    "        \"- Answer concisely and clearly in short paragraphs (2–6 lines each).\\n\"\n",
    "        \"- If examples or steps exist, use bullet points.\\n\"\n",
    "        \"- At the end, add a 'SOURCES' section listing page numbers if available.\\n\\n\"\n",
    "        \"ANSWER:\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Model (Google Gemini, you can switch to OpenAI if needed)\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"What is ARIMA model?\"\n",
    "result = qa_chain(query)\n",
    "\n",
    "# Extract answer & sources\n",
    "answer_text = result[\"result\"] if isinstance(result, dict) else result\n",
    "source_docs = result.get(\"source_documents\", []) if isinstance(result, dict) else []\n",
    "\n",
    "# Pretty print\n",
    "print(\"\\n📌 Answer:\\n\")\n",
    "print(textwrap.fill(answer_text.strip(), width=100))\n",
    "\n",
    "print(\"\\n📚 Sources:\")\n",
    "for i, doc in enumerate(source_docs, start=1):\n",
    "    page = doc.metadata.get(\"page\", \"Unknown\")\n",
    "    preview = doc.page_content.strip().replace(\"\\n\", \" \")[:300]\n",
    "    print(f\"{i}. Page: {page} — {preview}...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
