{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2455e8a5",
   "metadata": {},
   "source": [
    "## Important is chatpromt template and json foratter with pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3019b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the variables from the dot env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7a4ecf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic2.0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "725712ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0434d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tracking v2 has to be kept true as per documentation of langchain\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking and Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "922f09cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x1185411d0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11a0bb250> root_client=<openai.OpenAI object at 0x10efce850> root_async_client=<openai.AsyncOpenAI object at 0x118541090> model_name='gpt-4.1-nano-2025-04-14' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3916a3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"LangChain and OpenAI API are related tools that often work together to build advanced AI applications.\\n\\n**Here's how they relate:**\\n\\n1. **LangChain**: An open-source framework designed to develop applications that utilize large language models (LLMs). It provides abstractions for prompt management, memory, chains of prompts, and integration with various AI services, making it easier to build complex AI-powered systems.\\n\\n2. **OpenAI API**: A service offering access to OpenAI's powerful language models like GPT-4, GPT-3.5, and others. Developers use this API to generate text, perform completions, and perform other NLP tasks.\\n\\n**Their relationship:**\\n\\n- **Using OpenAI API within LangChain**: LangChain offers native integrations with the OpenAI API, allowing developers to easily incorporate OpenAI models into their applications. For example, you can create prompt templates, chain multiple API calls, and manage conversation state—all using LangChain’s abstractions, which under the hood call the OpenAI API.\\n\\n- **Enhanced application development**: Combining LangChain’s structure with OpenAI’s models allows for building sophisticated tools like chatbots, question-answering systems, language translation apps, and more, with streamlined code and added features like memory, prompt management, and orchestration.\\n\\n**In summary:**  \\nLangChain acts as a higher-level framework that simplifies using OpenAI’s language models, providing developers with tools to create complex, maintainable, and scalable AI applications that leverage OpenAI API’s language capabilities.\\n\\nIf you're building an AI app that uses OpenAI's models, LangChain can significantly accelerate development and add sophistication through its modular components.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 334, 'prompt_tokens': 14, 'total_tokens': 348, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_17d1f82c3e', 'id': 'chatcmpl-BazsXj9PTqw9NpDxhMr7IGBncM7l7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f201a0ec-64ea-4aab-bafc-f651ae58e2a7-0', usage_metadata={'input_tokens': 14, 'output_tokens': 334, 'total_tokens': 348, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how langchain and open api related\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb6c6786",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llm.invoke(\"how langchain and open api related\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01b89c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain and OpenAI APIs are related tools often used together to build advanced AI applications, particularly those involving natural language processing. Here's how they are connected:\n",
      "\n",
      "1. **Langchain Overview:**\n",
      "   - Langchain is an open-source framework designed to simplify the development of applications that utilize large language models (LLMs). It provides tools and abstractions for prompt management, memory, agents, chaining multiple calls, and integrating with various data sources.\n",
      "   - Its goal is to make it easier to build conversational AI, chatbots, question-answering systems, and more by managing complex workflows involving LLMs.\n",
      "\n",
      "2. **OpenAI API Overview:**\n",
      "   - The OpenAI API provides access to powerful language models like GPT-3, GPT-4, and others.\n",
      "   - Developers use the API to send prompts and receive generated text, enabling applications that require understanding, generation, summarization, translation, etc.\n",
      "\n",
      "3. **How They Work Together:**\n",
      "   - **Integration:** Langchain seamlessly integrates with the OpenAI API to invoke LLMs within a structured framework.\n",
      "   - **Simplified Workflow:** Instead of managing API calls manually, developers can use Langchain's abstractions—like chains, prompts, and agents—to build more complex and maintainable workflows.\n",
      "   - **Prompt Management:** Langchain helps design and manage prompts efficiently for OpenAI models, enhancing performance and reproducibility.\n",
      "   - **Memory & Context:** Langchain can maintain conversational state or context across multiple interactions with OpenAI models.\n",
      "   - **Multi-step Tasks:** Using Langchain, you can compose multiple API calls, incorporate external data, or implement logic that combines OpenAI's capabilities with other tools and data sources.\n",
      "\n",
      "**In summary:**  \n",
      "Langchain acts as a developer-friendly framework that leverages the OpenAI API's powerful language models to create sophisticated, multi-step, or context-aware AI applications more easily and efficiently. Together, they enable building scalable, maintainable NLP solutions with less complexity.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e75c2599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user said \"my name is sahil\". I need to respond appropriately. First, I should acknowledge their name. Maybe greet them and ask how I can help. Let me check the guidelines to make sure I\\'m following the right protocol. I should keep it friendly and open-ended. Let me make sure there\\'s no need for any specific action here, just a simple greeting. Alright, I\\'ll go with something like \"Hello, Sahil! How can I assist you today?\" That sounds good. It\\'s polite and invites them to state their request. I should avoid any markdown and keep the response natural. Yep, that should work.\\n</think>\\n\\nHello, Sahil! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 147, 'prompt_tokens': 15, 'total_tokens': 162, 'completion_time': 0.360933151, 'prompt_time': 0.0029058, 'queue_time': 0.24448737999999998, 'total_time': 0.363838951}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_512a3da6bb', 'finish_reason': 'stop', 'logprobs': None}, id='run--7c3aaefa-51c7-4ec4-98dc-5e8c627074ce-0', usage_metadata={'input_tokens': 15, 'output_tokens': 147, 'total_tokens': 162})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model= ChatGroq(model=\"qwen-qwq-32b\")\n",
    "model.invoke(\"my name is sahil\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ba46837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engieering\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "      (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "      (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f67d6754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x11a27d350>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x11a27d810>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model= ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ce7073e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x11a27d350>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x11a27d810>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining\n",
    "\n",
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97ed642f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI Engineer, I can definitely give you some information about LangSmith! \n",
      "\n",
      "LangSmith is an open-source tool developed by the team at AI21 Labs. Its primary purpose is to help you build and fine-tune large language models (LLMs) more efficiently. Think of it as a platform designed to streamline the process of working with LLMs.\n",
      "\n",
      "Here are some key features and aspects of LangSmith:\n",
      "\n",
      "**1. User-Friendly Interface:** LangSmith aims to make working with LLMs accessible to a wider audience, even those without extensive machine learning expertise. It provides a visual and intuitive interface for managing your projects and interacting with models.\n",
      "\n",
      "**2. Fine-tuning Capabilities:**  A major strength of LangSmith is its ability to fine-tune pre-trained LLMs. You can take an existing model and adapt it to your specific needs by training it on your own dataset. This allows you to create models that are specialized for tasks like:\n",
      "\n",
      "   * Text summarization\n",
      "   * Question answering\n",
      "   * Code generation\n",
      "   * Dialogue systems\n",
      "\n",
      "**3. Experiment Tracking and Management:** LangSmith helps you keep track of your fine-tuning experiments. You can easily compare different model architectures, hyperparameters, and datasets to see what works best for your use case.\n",
      "\n",
      "**4. Collaboration Features:** LangSmith supports collaboration, allowing multiple people to work on the same project simultaneously. This can be beneficial for research teams or organizations developing AI applications.\n",
      "\n",
      "**5. Open-Source Nature:** As an open-source project, LangSmith encourages community involvement and contributions. This means you can benefit from the collective knowledge and expertise of the AI community.\n",
      "\n",
      "**Where to Learn More:**\n",
      "\n",
      "* **Official Website:** [https://www.ai21.com/langsmith](https://www.ai21.com/langsmith)\n",
      "* **GitHub Repository:** [https://github.com/AI21-Studio/langsmith](https://github.com/AI21-Studio/langsmith)\n",
      "\n",
      "LangSmith is a powerful tool that can empower you to explore the potential of LLMs and build innovative AI applications.\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about LangSmith or anything else related to AI!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"can you provide me some info about langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da0629c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me tell you about LangSmith!  \n",
      "\n",
      "**LangSmith** is an open-source tool designed to simplify the process of fine-tuning large language models (LLMs) for specific tasks. \n",
      "\n",
      "Think of it as a user-friendly platform built on top of the powerful **LangChain** framework.  Here's a breakdown of its key features and benefits:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Streamlined Fine-Tuning:** LangSmith makes fine-tuning LLMs accessible even to those without extensive machine learning expertise. It provides a graphical interface and intuitive workflow, guiding you through the process.\n",
      "\n",
      "* **Data Management:** It offers robust tools for managing your training data, including data cleaning, preprocessing, and splitting into training, validation, and test sets.\n",
      "\n",
      "* **Experiment Tracking:**  LangSmith helps you track your fine-tuning experiments, making it easy to compare different model architectures, hyperparameters, and datasets.\n",
      "* **Community-Driven:** As an open-source project, LangSmith benefits from a vibrant community of developers and researchers who contribute to its development and share their knowledge.\n",
      "* **Integration with LangChain:**  LangSmith leverages the power of LangChain, a popular framework for building applications with LLMs. This integration allows you to easily incorporate your fine-tuned models into larger workflows.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Accessibility:** Reduces the barrier to entry for fine-tuning LLMs, empowering more individuals and organizations to leverage their potential.\n",
      "* **Efficiency:**  Simplifies the fine-tuning process, saving time and resources.\n",
      "* **Reproducibility:**  Makes it easier to reproduce and share fine-tuning results.\n",
      "* **Customization:**  Allows you to tailor LLMs to your specific needs and domains.\n",
      "\n",
      "**Use Cases:**\n",
      "\n",
      "* **Chatbot Development:** Fine-tune LLMs for more engaging and personalized chatbot experiences.\n",
      "* **Text Summarization:**  Create models that can accurately summarize large amounts of text.\n",
      "* **Code Generation:**  Train LLMs to generate code in specific programming languages.\n",
      "* **Question Answering:**  Build systems that can answer questions based on a given context.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "If you're interested in exploring LangSmith, check out the official documentation and community resources:\n",
      "\n",
      "* **Documentation:** [https://github.com/langsmithai/langsmith](https://github.com/langsmithai/langsmith)\n",
      "* **Community:** [https://discord.gg/LangSmith](https://discord.gg/LangSmith)\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any more questions about LangSmith or want to dive deeper into specific aspects of it!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### output parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response= chain.invoke({\"input\":\"can you tell me something about langsmith\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df32175",
   "metadata": {},
   "outputs": [],
   "source": [
    "### output parser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea0f50e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### output parser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"answer the user query \\n {format_instruction} \\n {query} \\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54148e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d003466",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"can you tell me about langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d23012",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do the above with chat prompt template and xml output parser"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
